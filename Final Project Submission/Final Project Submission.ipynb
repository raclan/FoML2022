{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ryanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import contractions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "from nltk.translate import meteor, meteor_score\n",
    "nltk.download('wordnet')\n",
    "import Levenshtein\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and convert data into dataframes\n",
    "training = open('train_with_label.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "temp = []\n",
    "for row in training:\n",
    "    temp.append(row.split('\\t'))\n",
    "\n",
    "traindf = pd.DataFrame(temp)[:-1]\n",
    "\n",
    "dev = open('dev_with_label.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "temp = []\n",
    "for row in dev:\n",
    "    temp.append(row.split('\\t'))\n",
    "\n",
    "devdf = pd.DataFrame(temp)[:-1]\n",
    "\n",
    "#Create string arrays for each dataset\n",
    "train_col1 = traindf.iloc[:,1]\n",
    "train_col2 = traindf.iloc[:,2]\n",
    "\n",
    "dev_col1 = devdf.iloc[:,1]\n",
    "dev_col2 = devdf.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the strings to remove punctation, remove common words, and lowercase\n",
    "def clean_string(text):\n",
    "    text = text.replace('â€™', '\\'')\n",
    "    text = contractions.fix(text)\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Define new features\n",
    "def new_features(col1, col2):\n",
    "\n",
    "    #Find Euclidean Distance\n",
    "    edist = []\n",
    "\n",
    "    #Find cosine similarity\n",
    "    csim = []\n",
    "\n",
    "    #Find lengths of sentences (words)\n",
    "    length1 = []\n",
    "    length2 = []\n",
    "\n",
    "    #Find slices of sentences (number of similar words between sentences)\n",
    "    slices = []\n",
    "\n",
    "    #Find meteor scores\n",
    "    mscores = []\n",
    "\n",
    "    #Find Levenshtein distance\n",
    "    ldists = []\n",
    "\n",
    "    #Find BLEU scores\n",
    "    bleu1 = []\n",
    "    bleu2 = []\n",
    "    bleu3 = []\n",
    "\n",
    "    #Find Levenshtein ratio\n",
    "    lratios = []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    for (sent1, sent2) in zip(col1, col2):\n",
    "\n",
    "        #Euclidean Distances\n",
    "        corpus = [sent1, sent2]\n",
    "        features = vectorizer.fit_transform(corpus).todense() \n",
    "        edist.append(euclidean_distances(features[0], features[1])[0][0])\n",
    "\n",
    "        #Cosine Similarities\n",
    "        vectors = vectorizer.fit_transform(corpus).toarray()\n",
    "        csim.append(cosine_similarity(vectors)[0][1])\n",
    "\n",
    "        #Lengths of sentences (words)\n",
    "        length1.append(len(sent1.split()))\n",
    "        length2.append(len(sent2.split()))\n",
    "\n",
    "        #Slices\n",
    "        slice = 0  \n",
    "        for word1 in sent1.split():\n",
    "            for word2 in sent2.split():\n",
    "                if word1 == word2:\n",
    "                    slice += 1\n",
    "        slices.append(slice)\n",
    "  \n",
    "        #Meteor Scores\n",
    "        mscore = meteor_score.meteor_score([sent1], sent2)\n",
    "        mscores.append(mscore)\n",
    "\n",
    "        #Levenshtein Distances\n",
    "        ldist = Levenshtein.distance(sent1, sent2)\n",
    "        ldists.append(ldist)\n",
    "\n",
    "        #Levenshtein Ratios\n",
    "        lratio = Levenshtein.ratio(sent1, sent2)\n",
    "        lratios.append(lratio)\n",
    "\n",
    "        #BLEU Scores\n",
    "        bscore1 = sentence_bleu([sent1.split()], sent2.split(), weights=[1])\n",
    "        bleu1.append(bscore1)\n",
    "\n",
    "        bscore2 = sentence_bleu([sent1.split()], sent2.split(), weights=[1/2, 1/2])\n",
    "        bleu2.append(bscore2)\n",
    "\n",
    "        bscore3 = sentence_bleu([sent1.split()], sent2.split(), weights=[1/3, 1/3, 1/3])\n",
    "        bleu3.append(bscore3)\n",
    "\n",
    "    #Find absolute value of difference between lengths\n",
    "    lengthdiff = abs(np.array(length2) - np.array(length1))\n",
    "\n",
    "    return edist, csim, length1, length2, lengthdiff, slices, mscores, ldists, lratios, bleu1, bleu2, bleu3\n",
    "\n",
    "#Create df with new features given two string arrays\n",
    "def new_df(col1, col2):\n",
    "    edist, csim, length1, length2, lengthdiff, slices, mscores, ldists, lratios, bleu1, bleu2, bleu3 = new_features(col1, col2) \n",
    "\n",
    "    return_df = pd.DataFrame()\n",
    "    return_df['edist'] = edist\n",
    "    return_df['csim'] = csim\n",
    "    return_df['length1'] = length1\n",
    "    return_df['length2'] = length2\n",
    "    return_df['lengthdiff'] = lengthdiff\n",
    "    return_df['slices'] = slices\n",
    "    return_df['mscores'] = mscores\n",
    "    return_df['ldists'] = ldists\n",
    "    return_df['lratios'] = lratios\n",
    "    return_df['bleu1'] = bleu1\n",
    "    return_df['bleu2'] = bleu2\n",
    "    return_df['bleu3'] = bleu3\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and dev sets with cleaning\n",
    "train1 = list(map(clean_string, train_col1))\n",
    "train2 = list(map(clean_string, train_col2))\n",
    "\n",
    "dev1 = list(map(clean_string, dev_col1))\n",
    "dev2 = list(map(clean_string, dev_col2))\n",
    "\n",
    "cleantrain = new_df(train1, train2)\n",
    "cleantrain['paraphrase'] = traindf[3]\n",
    "\n",
    "cleandev = new_df(dev1, dev2)\n",
    "cleandev['paraphrase'] = devdf[3]\n",
    "\n",
    "X_train = cleantrain.drop('paraphrase', axis=1)\n",
    "y_train = cleantrain['paraphrase']\n",
    "X_dev = cleandev.drop('paraphrase', axis=1)\n",
    "y_dev = cleandev['paraphrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8248106060606061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(75, 36, 2), alpha=0.00005, batch_size=250, random_state=9, activation=\"relu\")\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), model)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_dev)\n",
    "\n",
    "print(\"F1 Score: {}\\n\".format(f1_score(y_dev,y_pred,pos_label='1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open('test_without_label.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "temp = []\n",
    "for row in test:\n",
    "    temp.append(row.split('\\t'))\n",
    "\n",
    "testdf = pd.DataFrame(temp)[:-1]\n",
    "\n",
    "test_col1 = testdf.iloc[:,1]\n",
    "test_col2 = testdf.iloc[:,2]\n",
    "\n",
    "test1 = list(map(clean_string, test_col1))\n",
    "test2 = list(map(clean_string, test_col2))\n",
    "\n",
    "cleantest = new_df(test1, test2)\n",
    "\n",
    "test_pred = pipeline.predict(cleantest)\n",
    "\n",
    "finaldf = pd.DataFrame()\n",
    "finaldf['instance_id'] = testdf.iloc[:,0]\n",
    "finaldf['predicted_label'] = test_pred\n",
    "\n",
    "finaldf.to_csv('RyanAclan_test_result.txt', header=None, index=None, sep='\\t', mode = 'a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51060068504f0ac08f07c76779ded23f779c75b6ec98a50a3896701dbf655d52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
